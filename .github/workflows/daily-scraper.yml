name: Daily Article Scraper

on:
  schedule:
    # Run daily at 12:00 AM UTC (adjust timezone as needed)
    - cron: '0 */12 * * *'
  workflow_dispatch: # Allow manual triggering
    inputs:
      target_count:
        description: 'Number of articles to scrape'
        required: false
        default: '50'
        type: string
      log_level:
        description: 'Log level'
        required: false
        default: 'INFO'
        type: choice
        options:
          - DEBUG
          - INFO
          - WARNING
          - ERROR

# Prevent concurrent runs
concurrency:
  group: article-scraper
  cancel-in-progress: false

env:
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: 'v2'

jobs:
  scrape-articles:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ env.CACHE_VERSION }}-${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ env.CACHE_VERSION }}-${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create logs directory
      run: mkdir -p logs
      
    - name: Validate environment
      run: |
        echo "Python version: $(python --version)"
        echo "Pip version: $(pip --version)"
        echo "Dependencies installed:"
        pip list | grep -E "(requests|beautifulsoup4|pymongo|feedparser|python-dotenv)"
        
    - name: Test MongoDB connection
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
        MONGODB_DATABASE: ${{ secrets.MONGODB_DATABASE }}
        MONGODB_COLLECTION: ${{ secrets.MONGODB_COLLECTION }}
      run: |
        echo "üîç Testing MongoDB connection..."
        python -c "
        import os
        from pymongo import MongoClient
        from pymongo.errors import ConnectionFailure
        
        uri = os.getenv('MONGODB_URI', 'Not set')
        db_name = os.getenv('MONGODB_DATABASE', 'Not set')
        collection_name = os.getenv('MONGODB_COLLECTION', 'Not set')
        
        # Only show configuration status, not actual values
        print(f'MongoDB URI: {\"‚úÖ Configured\" if uri and uri != \"Not set\" else \"‚ùå Not configured\"}')
        print(f'Database: {db_name}')
        print(f'Collection: {collection_name}')
        
        if uri and uri != 'Not set':
            try:
                client = MongoClient(uri, serverSelectionTimeoutMS=5000)
                client.admin.command('ismaster')
                db = client[db_name]
                collection = db[collection_name]
                collection.count_documents({})
                print('‚úÖ MongoDB connection test successful')
                client.close()
            except ConnectionFailure:
                print('‚ùå MongoDB connection failed: Network/Authentication error')
                print('Check MongoDB URI, credentials, and network connectivity')
                exit(1)
            except Exception as e:
                print(f'‚ùå MongoDB test failed: {type(e).__name__}')
                print('Verify database configuration and permissions')
                exit(1)
        else:
            print('‚ùå MongoDB URI not configured')
            exit(1)
        "
        
    - name: Run article scraper
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
        MONGODB_DATABASE: ${{ secrets.MONGODB_DATABASE }}
        MONGODB_COLLECTION: ${{ secrets.MONGODB_COLLECTION }}
        TARGET_ARTICLE_COUNT: ${{ inputs.target_count || '50' }}
        LOG_LEVEL: ${{ inputs.log_level || 'INFO' }}
        AUTO_CLEANUP_ENABLED: true
        CLEANUP_MONTHS_OLD: 2
        RATE_LIMIT_DELAY: 1.5
        MAX_RETRIES: 3
      run: |
        echo "Starting article scraper with TARGET_ARTICLE_COUNT=$TARGET_ARTICLE_COUNT"
        python main.py
        
    - name: Check scraper output
      if: always()
      run: |
        echo "=== Scraper Results ==="
        if [ -f "articles_$(date +%Y%m%d).json" ]; then
          echo "‚úÖ JSON backup created: articles_$(date +%Y%m%d).json"
          echo "üìä Articles scraped: $(jq length articles_$(date +%Y%m%d).json 2>/dev/null || echo 'Unable to count')"
        else
          echo "‚ùå No JSON backup found"
        fi
        
        if [ -f "logs/scraper.log" ]; then
          echo "üìù Log file size: $(du -h logs/scraper.log | cut -f1)"
          echo "üîç Last 10 log entries:"
          tail -10 logs/scraper.log
        else
          echo "‚ùå No log file found"
        fi
        
    - name: Upload logs as artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs-${{ github.run_number }}-${{ github.run_attempt }}
        path: logs/
        retention-days: 14
        compression-level: 6
        
    - name: Upload JSON backup
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: articles-json-${{ github.run_number }}-${{ github.run_attempt }}
        path: articles_*.json
        retention-days: 90
        compression-level: 9
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "üö® Workflow failed!"
        echo "Check the logs above for details."
        echo "Common issues:"
        echo "- MongoDB connection problems"
        echo "- RSS feed timeouts"
        echo "- Network connectivity issues"
